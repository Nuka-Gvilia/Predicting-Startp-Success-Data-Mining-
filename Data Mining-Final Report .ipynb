{"cells":[{"cell_type":"markdown","id":"12ec7ddc","metadata":{"id":"12ec7ddc"},"source":["### Team 1 - Final Project Report"]},{"cell_type":"markdown","id":"5011c60d","metadata":{"id":"5011c60d"},"source":["![a0ec7244d2e91d185390eb01ca38b734.png](attachment:a0ec7244d2e91d185390eb01ca38b734.png)"]},{"cell_type":"markdown","id":"ca8024fe","metadata":{"id":"ca8024fe"},"source":["### 1. Frame the problem and look at the big picture"]},{"cell_type":"markdown","id":"02a84269","metadata":{"id":"02a84269"},"source":["Define the objective in business terms/How will your solution be used? \n","\n","Our team decided to look at data involving startups, investors and enterpreneurs, in order to gain a better understanding of the start up landscape. We envision our findings and our prediction tool being used by two different groups - investors/VC funds & startup enterpreneurs. We believe that investors can use the prediction tool when they are debating whether they should invest in a startup or not. Our tool can act as an analytical assistant during decision making for VC funds as we leverage a plethora of features in our model that predicts whether a startup will succeed or fail. Furthermore, enterpreneurs can use the tool to assess their own start up and leverage our findings regarding the importance of relationships and funding rounds to make decisions on how to expedite the growth of their start up. \n","\n","\n","\n"]},{"cell_type":"markdown","id":"9cde8d24","metadata":{"id":"9cde8d24"},"source":["What are the current solutions/workarounds (if any)?\n","\n","Currently, VC's and private equity funds use a lot of primary and secondary research done by analysts, as well as immense domain expertise to make decisions regarding funding/supporting start ups. We know our tool cannot replace this in depth knowledge that industry experts leverage in decision making, but we do think that it can act as a supplemental tool during the decision making process for VC's and startups.\n"]},{"cell_type":"markdown","id":"4a9496f6","metadata":{"id":"4a9496f6"},"source":["How should you frame this problem (supervised/unsupervised, online/offline, etc.)/How should performance be measured?\n","\n","Our project has both supervised and unsupervised components. We have used unsupervised techniques in order to cluster startups to gain a better understanding of the different profiles of start ups. We have employed supervised techniques such as logistic regression, random forests and GridSearchCV in order to create a tool that can predict whether a startup will be successful or not. Additionally, we have done in depth EDA to better understand the educational background of people involved in the start up space and the type of start ups that exist today. This will help enterpreneurs understand what the typical profile of people in the space is and also give some insight into which industries allow for the most growth and success.\n","\n","Since our model is classification based, we will focus on metrics such as accuracy scores, f1 scores and confusion matrices. If the model is actually utilized by VC's to assist with decision making, we can look at real life metrics such as return on investment, in order to see if our tool can really add value to the startup space. \n","\n"]},{"cell_type":"markdown","id":"425826a2","metadata":{"id":"425826a2"},"source":["What are comparable problems? Can you reuse experience or tools?\n","\n","As this is a classification problem with imbalanced classes, we can look at similar problems such as credit card fraud detection. This problem also has imbalanced classes with a small minority class of frauds. Hence, we can seek inspiration from the way this problem is solved as it is such a well researched area and apply those learnings to our problem relating to start up success prediction.\n"]},{"cell_type":"code","execution_count":null,"id":"8dcd1c89","metadata":{"id":"8dcd1c89"},"outputs":[],"source":[""]},{"cell_type":"markdown","id":"de20968f","metadata":{"id":"de20968f"},"source":["### 3 Explore the data"]},{"cell_type":"markdown","id":"82d92c66","metadata":{"id":"82d92c66"},"source":["We leveraged the overview feature which had a short description of each startup in order to create a wordcloud to better understand the types of startups we were dealing with. We used the textBlob and WordCloud packages to create this visual. Prior to passing the text into the wordCloud package, we did some text cleaning using regex to ensure that the wordCloud had meaningful results. We also leveraged the stopwords attribute to remove certain common words that did not give us any useful insights. \n","\n","\n","We found that technology, social media, social network, search engine and com were very common words. This suggested that we were dealing with a lot of tech startups relating to social media in our dataset. The word com also suggested that there were many ecommerce websites/startups in the data. Words such as mobile app and mobile application also reinforced the idea that many of our rows were tech related start ups. \n","\n","Words such as data, information,software and web based also reinforced this idea. "]},{"cell_type":"markdown","id":"f113bdae","metadata":{"id":"f113bdae"},"source":["![Screen%20Shot%202022-03-11%20at%2012.58.44%20PM.png](attachment:Screen%20Shot%202022-03-11%20at%2012.58.44%20PM.png) "]},{"cell_type":"markdown","id":"9d85dc83","metadata":{"id":"9d85dc83"},"source":["![download%20%2822%29.png](attachment:download%20%2822%29.png)"]},{"cell_type":"markdown","id":"64e71943","metadata":{"id":"64e71943"},"source":["While exploring the features for start up success prediction, we found some interesting results to share. \n","\n","We found that companies that have an IPO need significantly more relationships than other startups. This emphasizes truly how difficult it is for startups to achieve an IPO and how complicated the process can be as it requires a variety of expertise in different areas. \n","\n","We also found that companies that closed or had an IPO, on average, had the same number of funding rounds. This was interesting as it suggests that numerous funding rounds is not always a good thing. It shows that some startups can get overwhelmed by excessive funding and this can cause them to fail. On the other hand, others might be able to leverage the funds appropriately in order to fuel growth. "]},{"cell_type":"markdown","id":"4cf686fc","metadata":{"id":"4cf686fc"},"source":["![Screen%20Shot%202022-03-11%20at%201.12.46%20PM.png](attachment:Screen%20Shot%202022-03-11%20at%201.12.46%20PM.png)"]},{"cell_type":"markdown","id":"6b859f06","metadata":{"id":"6b859f06"},"source":["We then did a deep dive on the types of startups in our data set. We found that software, web, eccomerce and games_video startups were the most common. This reinforced our findings from the wordCloud we had created previously. \n","\n","Following a similar logical trend, we found that these industries had the highest levels of failed startups as well. This is probably due to the excessive competition in this space and the oversaturation of the market and the fierce battle to acquire the same customers. \n","\n","Interestingly, we found that biotech startups had the most success in achieving an IPO. We expected software to be at the top of the list so this was an interesting observation. \n","\n","Finally, we looked at the failure rate of start ups in different industries. We found that industries such as security, advertising and music lead the pack in failure rate. The music industry made sense as there are a few dominant players in the space so any new entrant would face immense challenges to stay in business and acquire a sustainable amount of market share. \n","\n"]},{"cell_type":"markdown","id":"86dd97b7","metadata":{"id":"86dd97b7"},"source":["![download%20%2824%29.png](attachment:download%20%2824%29.png)"]},{"cell_type":"markdown","id":"78d8ccc4","metadata":{"id":"78d8ccc4"},"source":["### 4. Prepare the data (For startup success prediction)"]},{"cell_type":"markdown","id":"684abd3f","metadata":{"id":"684abd3f"},"source":["Data cleaning:  \n","\n","We started by filtering the objects data set to make it appropriate for our analysis. As our target variable was the 'status' feature, we first looked at the value_counts of the each category in status and found that we wanted to focus on the four main categories - IPO, acquired, operating & closed. We filtered the dataset to only include rows with these categories and hence removed about 1200 rows. \n","\n","We then noticed that the dataset consisted of people and firms. As we were focused on start up success predicition, we filtered the data set to only include the rows which referenced companies/start ups. \n","\n","Following this, we investigated the various features present in the data set and found that there were numerous redundant/not useful columns that contained references to how the data was colleced. Hence, we decided to drop those columns and make our data set more compact and readable. \n","\n","Additionally, we used the msno package to visualize the missing values in each feature. This helped us make decisions regarding which features to drop due to vary sparse data population. Through this analysis, we dropped columns such as 'short_description' and 'closed_at' \n","\n","We then focused on the numerical variables. We first did some winsorizing to remove major outliers in features such as relationships, milestones and funding_total_usd. Following that, we looked at the distribution of the numerical features and performed box cox transformations to make the distributions more normal in nature. After that, we used a StandardScaler() to scale the data and make it useable for our modeling stage. In the future, we could also experiment with different types of Scalers and see how using different scalers affects the performance of our classification model. \n","\n","Post cleaning the numerical variables, we shifted our focus to the categorical features. These features did not have any missing values so we could directly start working with them without any imputation. We decided to use a labelEncoder to make our categorical features useable for the model building. We made the decision to use the labelEncoder instead of one hot encoding because we felt that one hot encoding would result in the creation of too many additional features that might cause overfitting when we train our data. So, we made the choice of using a labelEncoder. \n","\n","Following this, we decided to do some feature engineering to make our target variable useable. We decide to have two categories for our predictor - success & failure. So we assigned any row with status as IPO, acquired or operating as success and gave it the value 1. We assigned a 0 to any row with status as closed. \n","\n","Finally, as this is a classification problem, we looked at the balance between the two classes of our target variable. We found that the target variable was highly imbalanced. The majority class was the success class (1's) and the minority class was the fail class (0's). In order to resolve this imbalance, we used the RandomOverSampler package and the SMOTE package in order to synthetically create more rows of the minority class and balance the data set and ensure that it is ready to use.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","id":"900fb864","metadata":{"id":"900fb864"},"source":["### 5. Short-list promising models/Fine-Tune the System"]},{"cell_type":"markdown","id":"fc1626c5","metadata":{"id":"fc1626c5"},"source":["We chose to use clustering as a way to gain understanding of company profiles. Before running the clustering algorithm, we decided to separate our data by macroregions and US regions since the US made up the majority of the data we had. We used k-modes clustering because our data was all categorical. Using two separate elbow plots for the regions we defined, we found the optimal number of clusters to use to be 8. \n","\n","While there was a variation between funding round types and category codes shown between clusters, we saw that the region is predominantly North America, which suggested that most startups, at least within this dataset, are based in and operated there. From this we also drew some insight as to how market segments can vary across regions. For example, software appeared twice as the most common category in that cluster (clusters 1 & 2). However, they had different regions and funding type that suggested that the software industry in North America seemed to be more mature than Europes, since the most frequent funding round type was Series-b compared to Venture (which varies more and doesn't have defined series). We can say that perhaps the opposite is true when comparing startups in the biotech industries between Europe and North America (clusters 3 & 5). However, since \"other\" doesn't provide much information, we can't say for sure this is the case.\n","\n","We saw that from cluster 4, North America seems to appreciate riskier ventures particularly in the web category since \"angel\" was the most common type of funding. From cluster 7, we saw that startups in the enterprise category are usually more mature, due to the series-c+ dominant funding.\n","\n","It seemed to be the standard for most startups in clusters 2, 3, and 4 to sell their company and become acquired as opposed to clusters 0 and 7 where there was a more even ratio between startups who decide to go public and startups that get acquired. This could be because of the predominant region or category present in each cluster.\n","\n","In the US, compared to worldwide cluster membership, it seemed that again, 2 and 4 prefer to be acquired rather than going public. However, we saw that they both tend to follow the same pattern, suggesting that North America accounted for most of the trends being shown, since it is a large portion of our dataset.\n"]},{"cell_type":"markdown","id":"18f9e8ec","metadata":{"id":"18f9e8ec"},"source":["We started by building a base model to use as a reference. We imported the logisticRegression package and fit the model on our training data and following that we used the predict function to get our predictions on the test set. Throughout our analysis, we used an accuracy score as well as a confusion matrix to assess the performance of the model. Our initial logistic regression model had an accuracy of 0.80 on our test data. This was the level achieved by using the randomly oversampled dataset. We then tried using the SMOTE data set on the same model and found that our accuracy had improved to 0.84. Due to this finding, we decided to use the SMOTE data set for the rest of the models we ended up training. We also investigated the significant features from our model and found that funding_rounds, relathionships, funding_total_usd were the most significant. In the future, we would like to do indepth backward and forward selection to fine tune our models further and drop features that are not significant predictors. Following this, we decided to do gridsearchCV on our logistic regression model and found that using an l1 penalty instead of the default l2 slightly increased performance of the model. \n","\n","For our second model, we decided to use randomForests. We imported the randomForestClassifier and began training our first model. We found that using a random forest significantly improved performance and gave us an accuracy score of 0.9678 with far fewer false positives and false negatives. We then analysed the significant features of this model using the mean decrease in impurity metric and found that although the top 3 features were the same as the previous model, their relative importance was different. We did find the big jump in accuracy slightly unusual so we decided to run a bagging classifier and ADAboost using random forests on our SMOTE data. With ADABoost, we got an accuracy of 0.91 and with the bagging classifier, we got an accuracy of 0.87. Although the accuracy was lower, we believe that these models might be more viable to use in reality as it is possible that our randomForestClassifier is overfitting and will not generalize well in production. This can be seen as a more generalized model with bagging results in lower accuracy. Hence, this is an important aspect to consider when deciding on which model to select as there can be a trade off between accuracy and the ability of a model to generalize well. \n","\n","Following this, we used a VotingClassifier with the attribute voting = soft. We used both the random forest and logistic regression in this model. We found a slightly lower accuracy than just using a random forest and we got a value of 0.94. This might be due to the underperforming logistic regression model. \n","\n","Finally, we experimented with LinearSVC just to get a better idea of the package and get some practice with the implementation. Unfortunately, this model performed very poorly and gave us an accuracy of 0.576.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"65dbffc1","metadata":{"id":"65dbffc1"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"Data Mining-Final Report .ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}